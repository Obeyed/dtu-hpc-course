\section{Introduction to Parallelism}
\label{sec:introduction to parallelism}
The goal of using GPGPUs is to solve computational problems, either faster or bigger than what was previously possible with CPUs.
Often a simple port of a piece of code, given it has a parallel nature, will resolve in a significant speed-up (x3-x5).
However, utilizing the software and hardware it is possible to go way beyond the initial speed-up.
The principels of GPGPU programming when coding for efficiency is to maximize usefull operations by minimizing memory bottlenecks and divergence that forces waiting time amongst threads.

It is usefull to abstract the optimization to different levels as some optimizations might be more usefull than others.

\begin{enumerate}
\item Picking good algorithms
\item Basic principles for efficiency
\item Arch-specific detailed optimizations
\item Micro optimizations
\end{enumerate}

The single most important element in optimization is to pick an algorithm with strong asymptotic bounds.
Optimizing insertion sort $O(n^2)$ as opposed to merge-sort $O(nlgn)$ would make even a naive implementation of merge-sort vastly supperior to a very optimized version of insertion sort.
We will discuss algorithmic specific for parallel coding in \cref{sec:software optimisations}

Further, when considering algorithms for parallel purposes, the parallizability in an algorithm is of importance.
When designing algorithms we will view the computations as a directed acyclic graph(DAG).
This graph will have a set of computational steps linked togetther from top till bottom such as illustrated in figure. \todo{cref}
The important aspect is the workload to step ratio. The workload decides the total amount of work nessesary for that specific algorithm whereas the step size determinez the amount of serial work in the algorithm.
For an algorithm to have a parallel nature the step size has to be relatively low compared to the workload.
When we describe the algorithms we have computed in the algorithmic section we will also describe the algorithms work load and step size.

Basic principles for efficieny is the second most important aspect.
Developing cache aware kernels that utilizes the cache efficiently is critical to reduce memory bottlenecks.
In \cref{sec:memory optimisations} we will provide tricks to optimize the memory transfers and utilize faster memory.

Architectural specific optimization concerns utilizing the given architecture of a specific GPGPU's SM such as amount of threads per SM, L1 cache size for shared memoryetc.
In \cref{sec:hardware optimisations} we will consider how to utilize the GPGPU architecture when setting kernel parameters.

Micro level optimization works at the bit-level, such as approximating the inverse of a square root with magic numbers.
We will not consider these types of optimizations on the GPGPU in this report as the gains are minimal compared with the other optimisations \cite{udacity}.
