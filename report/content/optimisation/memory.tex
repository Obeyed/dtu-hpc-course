\section{Memory Optimisations}
\label{sec:memory optimisations}

This section presents relevant memory optimisations.

\subsection{Coalesced Memory Access}
\label{sec:coalesced}

In \cref{sec:gpgpu memory architecture} we introduced the L1 cache, which is designed for spatial locality.
Spatial locality promotes the use of coalesced memory access by the individual threads.
Coalesced memory access means to have threads access successive memory locations.
Since querying the global memory for a data value will often cause a memory transfer larger than what the thread requires and place the data in the faster cache on the SM.
When the subsequent thread queries for the successive value it will be returned from the L1 cache instead of the global memory, giving a substantial speed up.

In order to utilize coalesced the memory layout the data needs to be in a structure that promotes data to be successive rather than strided.~\cite{udacity}
This leads to two different approaches for storing data, namely array of structures (AoS) and structures of arrays (SoA), exemplified with RGB images in \cref{lst:soa aos}.

\begin{lstlisting}[caption={Example of SoA and AoS with RGB images}, label={lst:soa aos}]
struct {
  int R, G, B;
} AoS[N];

struct {
  int R[N], G[N], B[N];
} SoA;
\end{lstlisting}

The AoS lays out the images in the following manner

\begin{quote} 
  \ttt{R[1], G[1], B[1], R[2] G[2], B[2] $\ldots$ R[N], G[N], B[N]}
\end{quote}

whereas the SoA would lay out the images in the following manner

\begin{quote}
  \ttt{R[1], R[2], $\ldots$ R[N], G[1], G[2], $\ldots$ G[N], B[1], B[2], $\ldots$ B[N]}
\end{quote}

The SoA thus manages to lay out the data in a coalesced fashion, whereas the AoS lays out the data in a strided fashion.

\subsection{Shared Memory}
\label{sec:shared memory}

In \cref{sec:grids blocks threads} we introduced blocks as having some amount of shared memory available.
Shared memory is a defined space of memory in the SM's L1 cache that a set of threads within a block can read/write to during block execution.
The memory will disappear when the block terminates.
In this report we will consider two types of instances where shared memory could give a beneficial speed up.

The first is when a set of threads have to perform several read/write operations to a fixed set of global memory, which we as use in \cref{sec:reduce}.
By mapping the global memory to shared memory, computing the set of operations in shared memory and then writing back to global memory allows us utilize the fast access to shared memory.

The second is to coalesce writes by collecting them in shared memory.
This type of coalescing is especially interesting in the transpose problem where data is read row-wise but written column-wise. 
In \cref{sec:transpose} will introduce how we used tiles in shared memory to allow coalesced writing.

\subsection{Bank conflicts}
\label{sec:bank conflicts}

The local memory is divided into memory banks.
Transactions within these memory banks can only be accessed in a serial manner.
Thus, if the memory bank is busy and a warp requests some data, the data retrieval must wait its turn.
This is called a bank conflict.
Such conflicts can be avoided if the memory access are successive and do not cross other warp's data requests there will be no bank conflict.~\cite{farber2011cuda}
