\section{Map}
\label{sec:map}

As introduced in \cref{sec:challenges with parallel programs} we present a serial execution of the mapping operation and a parallel implementation of the mapping operation.
The serial code for a mapping operation is presented in \cref{lst:map seq}.
The code loops through each index of the input and copies the value to the same index in the output.
Given $n=\mathtt{ARRAY\_SIZE}$ this algorithm will have a workload of $\mathcal{O}(n)$, and step size of $\mathcal{O}(n)$.

\begin{lstlisting}[caption={Serial map}, label={lst:map seq}]
void map(int *h_in, int *h_out, int SIZE) {
  for (int j = 0; j < SIZE; j++) 
    h_out[j] = h_in[j];
}
\end{lstlisting}

As we can analyse the problem as a mapping operation, as introduced in \cref{sec:challenges with parallel programs}, we know that every mapping operation is independent from one another why the amount of steps can be reduced to $\mathcal{O}(1)$ and the problem thus becomes 100\% parallelisible.
Given Amdahl's law we should be able to achieve a speed up in the amount of cores, which is 2880 according to \cref{ap:tesla k40 specifications}.
We present a simple kernel that perform such a map operation in \cref{lst:map par}.

\begin{lstlisting}[caption={Map kernel}, label={lst:map par}]
__global__ 
void map_kernel(int *d_out, int *d_in, int SIZE) {
  int mid = threadIdx.x + blockDim.x * blockIdx.x;
  if (mid >= SIZE) return;
  d_out[mid] = d_in[mid];
}
\end{lstlisting}

First the kernel calculates the index of thread as a product of its position in the block and grid.
Next, the kernel validates that the index is inside the bounds of the arrays it is operating on.
Finally, the input is copied to the new output.

We present the comparison of the map algorithms in \cref{fig:map plot}.
It is clear that the parallel mapping algorithm has an advantage.
All the operations can be performed in parallel, while the CPU has to perform every operation as consecutive operations.

In \cref{tab:mapping cpu vs gpgpu} we list the performance at $n = 2^{29}$.
However, the result is nowhere near the prediction of Amdahl's law, which said that we would get a 2880x speed up.
The reason is that the mapping operation is an operation with very low computational complexity and many expensive transfers to and from global memory.
Thus, the global memory transactions are the bottleneck.

\begin{table}[htb]
  \centering
  \begin{tabular}{c r | r r r r}
    \toprule
    device & algorithm & time in ms & speed up & bandwidth usage & usage percentage\\
    \midrule
    CPU & serial  & 1200.73 &  &  &  \\
    GPGPU & parallel & 25.32 & x47.42 & 169.63 GB/s & 58.9\% \\
    \bottomrule
  \end{tabular}
  \caption{Global vs. Shared memory read and writes}
  \label{tab:mapping cpu vs gpgpu}
\end{table}

At $n = 2^{29}$ it takes the CPU $1200.73 ms$ while the GPGPU computes the mapping in $25.32$ being $47.42$ times faster.

\begin{figure}[htb]
  \centering
  \input{graphics/plots/map}
  \caption{Runtime development of the map algorithms}
  \label{fig:map plot}
\end{figure}
