\section{Map}
\label{sec:map}

As introduced in \cref{sec:challenges with parallel programs} we present a CPU sequential execution of the mapping operation and a GPGPU accelerated parallel implementation of the mapping operation.
The serial code for a mapping operation is presented in \cref{lst:map seq}.
The code loops through each index of the input and copies the value to the same index in the output.
Given $n=ARRAY\_SIZE$This algorithm will take.
\begin{equation*}
workload: O(n)
Steps: O(n)
\end{equation*}

\begin{lstlisting}[caption={Serial map}, label={lst:map seq}]
void map(int *h_in, int *h_out, int SIZE) {
  for (int j = 0; j < SIZE; j++) 
    h_out[j] = h_in[j];
}
\end{lstlisting}

As we can analyse the problem as a mapping operation, as introduced in \cref{sec:challenges with parallel programs}, we know that every mapping operation is independant from one another why the amount of steps can be reduced to $O(1)$ and the problem thus becomes 100\% parallizible.
Given Amdahls law we should be able to achieve a speed up in the amount of cores, which is 2880 according to \cref{ap:tesla k40 specifications}

We present a simple kernel that perform such a map operation in \cref{lst:map par}.

\begin{lstlisting}[caption={Map kernel}, label={lst:map par}]
__global__ 
void map_kernel(int *d_out, int *d_in, int SIZE) {
  int mid = threadIdx.x + blockDim.x * blockIdx.x;
  if (mid >= SIZE) return;
  d_out[mid] = d_in[mid];
}
\end{lstlisting}

First the kernel calculates the index of thread as a product of its position in the block and grid.
Next, the kernel validates that the index is inside the bounds of the arrays it is operating on.
Finally, the input is copied to the new output.

We present the comparison of the map algorithms in \cref{fig:map plot}.
It is clear that the parallel mapping algorithm has an advantage.
All the operations can be performed in parallel, while the CPU has to perform every operation as consecutive operations.

At $n = 2^{29}$ it takes the CPU $1200.73 ms$ while the GPGPU computes the mapping in $25.32$ being a $x47.42$ speed-up.
However, Amdahls law predicted a $x2880$ speed-up, but as the mapping operation is an operation with very low computational operation and many expensive transfers to and from global memory we are held back by the memory bottlenecks.

\begin{figure}[htb]
  \centering
  \input{graphics/plots/map}
  \caption{Runtime development of the map algorithms}
  \label{fig:map plot}
\end{figure}
