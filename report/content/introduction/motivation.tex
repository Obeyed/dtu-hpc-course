\section{Motivation}
\label{sec:motivation}

Forty years ago, Gordon E. Moore predicted that the number of transistors will double every two years. Until now this prediction has been correct and is widely known as ``Moore's Law''. Previously, increasing the transistors and thus computational power was achieved by making smaller and faster transistors, which in turn was utilized for higher clock speed. However, within the last decade we have reached the ceiling for clock speed due to power and heat challenges. The current approach has thus been focussing on creating many simple and power efficient computational units and utilizing these in parallel.~\cite{udacity, schaller1997moore,bryant2003computer}

The performance of processors is often divided into two areas, latency and throughput. Latency is the speed of which a single task can be completed, whereas throughput is the total amount of computation produced over a given time frame.~\cite{farber2011cuda}

The two different approaches of building processors translates to the Central Processing Unit (CPU) and General Purpose Graphics Processing Unit (GPGPU). The CPU is focussed on reducing latency by having a few complex processors maximising clock speed for computing. The GPGPU on the other hand is designed for optimizing throughput using many simple and economical processors. However, the theoretical throughput that GPGPUs is based on the ability to utilize all of its simple processors at once.

To fully exploit the potential of the GPGPU a task has to be parallelisable and instructions developed with parallelisation in mind, especially as the memory structure of current GPGPUs are often the bottleneck in reaching the theoretical throughput.

The applicational interests, which is one of our reasons for taking this course, are often based on tasks requiring linear algebra of massive matrices. Linear algebra is by nature a very parallelisable task as the problem can be reduced to smaller parallel subproblems using divide-and-conquer methods~\cite{amdahlorgustafson2011,chen2014data}. As linear algebra has been the corner stone of machine learning, numerical analysis and graphics for decades we find the challenge of developing parallel algorithms for GPGPUs appealing, why we are taking this course.
