\section{Grids, Blocks, and Threads}
\label{sec:grids blocks threads}

\todo{merge with section 2.3}
In \cref{sec:gpgpu memory architecture} we introduced thenotion of grids, blocks and threads.


The computation on GPGPU is done through the use of "threads".
Threads are scheduled in ``warps'' by ``blocks'', where the blocks are organized in ``grids''.
A thread is an independant sequence of work where the sequence is defined by instructions of the kernel.
A thread has an ID, being a the given threads number in a set of threads, which it uses to coordinate its work without exchange of information between other processors.
\todo{warp}.
To organize threads and utilize the ability to communicate between threads on an SM where memory access is fast, the concept of blocks has emerged.
A block contains a set amount of threads, has a set amount of L1 cache on the SM reserved and is able to synchronize between threads at any point along executing a kernel.
It further ensures that every thread is runned before terminating the block.
As a single block might not be able to carry out all work in a job, a grid is defined to numerate the amount of blocks that are required to support computation of the job.
However, the grid does does not support the same type of operations as the block, such as synchronization at a specific point in the kernel or fast shared memory.
The reason is that a grid might contain more blocks than a single SM can handle, why shared memory would not make sense.
\todo{still needs some more shizzle}

 Threads are scheduling of work on a GPGPU is done through the usage computed through


From a development perspective the position of each block within the grid and each thread within its block is fetched with the built-in variables presented in \cref{tab:built-in variables}, which all have three dimensions (\ttt{x, y, z}).
\todo{nice to have illustration of grids of blocks of threads}

\begin{table}[htb]
  \centering
  \begin{tabular}{lll}
    \toprule
    variable & explanation & type \\
    \midrule
    \ttt{gridDim}   & dimension of grid           & \ttt{dim3}  \\
    \ttt{blockIdx}  & block's index within grid   & \ttt{uint3} \\
    \ttt{blockDim}  & dimension of block          & \ttt{dim3}  \\
    \ttt{threadIdx} & thread's index within block & \ttt{uint3} \\
    \bottomrule
  \end{tabular}
  \caption{Built-in variables that are only valid within functions that are executed on device}
  \label{tab:built-in variables}
\end{table}

These built-in variables are of type \ttt{uint3} and \ttt{dim3}, which are integer vectors.
The variables are used to specify dimensionality and receive position when running a kernel.
When constructing the \ttt{dim3} variable all unspecified components are set to 1.
As presented in \cref{sec:hardware specific numbers} the max number of threads per block is 1024 for the device we will be working with.~\cite{nvidia2015doc}

\subsection{Testing Warp Sizes}
\label{sec:testing warp sizes}

\todo{put in optimization section}

In \cref{sec:gpu} we introduced the concept of a warp, a scheduler for launching threads within a kernel.
\cref{tab:warp size testing} shows the average elapsed time over 1,000 runs for launching different threads, to see the impact compared to the amount of warps are needed.
One warp launches 32 threads at a time, asserted from \cref{ap:tesla k40 specifications}.

\begin{table}[htb]%
  \begin{minipage}{0.49\linewidth}
    \centering
    \begin{tabular}{lrr}
      \toprule
      threads (\#) & time (ms) & latency \\
      \midrule
      $32 \times 10$    & $5.321$ &          \\
      $32 \times 10-1$  & $5.336$ & $+0.28\%$  \\
      $32 \times 10-16$ & $5.600$ & $+5.24\%$  \\
      $32 \times 10-31$ & $5.892$ & $+10.73\%$ \\
      \bottomrule
    \end{tabular}
  \end{minipage}%
  \begin{minipage}{0.49\linewidth}
    \centering
    \begin{tabular}{lrr}
      \toprule
      threads (\#) & time (ms) & latency \\
      \midrule
      $32 \times 9$     & $5.561$ & \\
      $32 \times 9-1$   & $5.585$ & $+0.43\%$  \\
      $32 \times 9-16$  & $5.892$ & $+5.95\%$  \\
      $32 \times 9-31$  & $6.236$ & $+12.13\%$ \\
      \bottomrule
    \end{tabular}
  \end{minipage}%
  \caption{Testing different warp sizes}
  \label{tab:warp size testing}
\end{table}

We present the multiple of 32 as the base case for the two tables.
By using block size between $32 \times 8$ and $32 \times 9$ the SM still launched the same amount of warps (9 warps per block), but with idle threads in the 9'th warp as it must have 32 threads.
As the amount of threads per block is reduced the total amount of blocks nessesary to complete the job must increase and thus increases runtime.
\cref{tab:warp size testing} shows how the performance deteriorates as the amount of idle threads in the last warp increases.
