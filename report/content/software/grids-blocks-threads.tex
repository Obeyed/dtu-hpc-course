\section{Grids, Blocks, and Threads}
\label{sec:grids blocks threads}

In \cref{sec:gpgpu memory architecture} we introduced the notion of grids, blocks and threads.
From a development perspective the position of each block within the grid and each thread within its block is fetched with the built-in variables presented in \cref{tab:built-in variables}, which all have three dimensions (\ttt{x, y, z}).
\todo{nice to have illustration of grids of blocks of threads}

\begin{table}[htb]
  \centering
  \begin{tabular}{lll}
    \toprule
    variable & explanation & type \\
    \midrule
    \ttt{gridDim}   & dimension of grid           & \ttt{dim3}  \\
    \ttt{blockIdx}  & block's index within grid   & \ttt{uint3} \\
    \ttt{blockDim}  & dimension of block          & \ttt{dim3}  \\
    \ttt{threadIdx} & thread's index within block & \ttt{uint3} \\
    \bottomrule
  \end{tabular}
  \caption{Built-in variables that are only valid within functions that are executed on device}
  \label{tab:built-in variables}
\end{table}

These built-in variables are of type \ttt{uint3} and \ttt{dim3}, which are integer vectors.
The variables are used to specify dimensionality and receive position when running a kernel.
When constructing the \ttt{dim3} variable all unspecified components are set to 1.
As presented in \cref{sec:hardware specific numbers} the max number of threads per block is 1024 for the device we will be working with.~\cite{nvidia2015doc}

\subsection{Testing Warp Sizes}
\label{sec:testing warp sizes}

In \cref{sec:gpu} we introduced the notion of a warp.
\Cref{tab:warp size testing} shows the average elapsed time over 1,000 runs for launching different threads, to see the impact compared to the amount of warps are needed.
One warp launches 32 threads at a time.

\begin{table}[htb]%
  \begin{minipage}{0.49\linewidth}
    \centering
    \begin{tabular}{lrr}
      \toprule
      threads (\#) & time (ms) & latency \\
      \midrule
      $32 \times 10$    & $5.321$ &          \\
      $32 \times 10-1$  & $5.336$ & $+0.28\%$  \\
      $32 \times 10-16$ & $5.600$ & $+5.24\%$  \\
      $32 \times 10-31$ & $5.892$ & $+10.73\%$ \\
      \bottomrule
    \end{tabular}
  \end{minipage}%
  \begin{minipage}{0.49\linewidth}
    \centering
    \begin{tabular}{lrr}
      \toprule
      threads (\#) & time (ms) & latency \\
      \midrule
      $32 \times 9$     & $5.561$ & \\
      $32 \times 9-1$   & $5.585$ & $+0.43\%$  \\
      $32 \times 9-16$  & $5.892$ & $+5.95\%$  \\
      $32 \times 9-31$  & $6.236$ & $+12.13\%$ \\
      \bottomrule
    \end{tabular}
  \end{minipage}%
  \caption{Testing different warp sizes}
  \label{tab:warp size testing}
\end{table}

We present the multiple of 32 as the base case for the two tables.
By subtracting one and launching one thread less, the SM still launches an amount of threads that are a multiple of 32 and we have one idle thread.
By subtracting 31, we have 31 idle threads.
Our tests show that having idle threads actually increases the total latency.

