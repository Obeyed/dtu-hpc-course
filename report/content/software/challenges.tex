\section{Challenges with Parallel Programs}
\label{sec:challenges with parallel programs}

Parallel computing has a number of challenges related to cuncurrency such as deadlock and race conditions, which we will discuss in this section.

A program is in a deadlocked state if two or more threads are waiting for the other to finish, resulting in neither ever finishing.
This results in a program that will never advance and is thus considered being in a ``deadlock''.
A race condition is a situation where two or more threads attempt to perform two or more operations at the same time, where these operations must be performed in a proper order to be correct.
For instance, simultaneous update operations to the same memory address will cause a race condition.~\cite{farber2011cuda}

To give an example on how a race condition arises we present a naive implementation of a histogram in \cref{lst:naive histo}.
In the naive histogram we have no guarantee that the value of \ttt{d\_bins} have not changed between reading the value at line 9 and writing to it at line 10.
With no guarantee of having a correct representation of the bin, the thread might make make an uninteded write to the array as many other threads could have tried to increment inbetween.

In order to handle this race condition we need to utilize the synchronization tools given through CUDA.

\begin{lstlisting}[caption={Naive histogram implementation with race condition}, label={lst:naive histo}]
__global__
void naive_histogram_kernel(int *d_bins, const int *d_in, const int SIZE) {
  int tid  = threadIdx.x + blockDim.x * blockIdx.x;
  // fetch item from input
  int item = d_in[tid];
  // compute where to put item wrt. bin count
  int bin  = item % SIZE;
  // update amount for that bin
  current_value = d_bins[bin];
  d_bins[bin] = current_value + 1;
  // atomicAdd(&(d_bins[bin]), 1);
}
\end{lstlisting}

One solution to this problem is to use the atomic function where the given memory is locked while a thread is updating it and unlocked when the thread has completed the update.
By replacing line 9 \& 10 with line 11 the kernel will execute an atomic operation.
The atomic operation has a slight overhead, but the cost is small\todo{get some costs!}~\cite{udacity}.
The issue with using atomic is that it will force a sequential access to a memory slot.
In the worst case, when size of bin is one, the GPGPU is forced to act sequential.

Another solution is to us a reduce algorithm to avoid the \ttt{atomicAdd()}.
Here each thread will manage a local copy of the histogram, and combine these histograms two and two, to end up with a complete histogram from the entire amount of launched threads.
We present reduce algorithm in \cref{sec:reduce} and a reduce\_histogram which we will benchmark with the atomic histogram.
